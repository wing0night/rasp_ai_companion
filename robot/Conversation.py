import pyaudio
import webrtcvad
import numpy as np
import time
import re
from openai import OpenAI
import os
from mem0 import MemoryClient
from mem0 import Memory
from dotenv import load_dotenv
from Camera import Camera
import speech_recognition as sr
import wave

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

class Conversation:
    def __init__(self, device_index):
        self.vad = webrtcvad.Vad(2)  # ä½¿ç”¨æ¨¡å¼2ï¼Œé€‚ç”¨äºè¯­éŸ³èŠå¤©
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(format=pyaudio.paInt16,
                                  channels=1,
                                  rate=16000,
                                  input=True,
                                  input_device_index=device_index,
                                  frames_per_buffer=320)  # 320msçš„å¸§å¤§å°
        self.is_speaking = False
        self.recording = []
        self.recognizer = sr.Recognizer() # è¯­éŸ³è¯†åˆ«å™¨
        self.mem0_client = MemoryClient(api_key=os.environ.get('MEM0_API_KEY'))
        self.openrouter_client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=os.environ.get('OPENROUTER_API_KEY'),
        )
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.camera = Camera(device_id=0)

    def listen(self):
        while True:
            data = self.stream.read(320)  # è¯»å–320msçš„éŸ³é¢‘æ•°æ®
            if self.vad.is_speech(data, 16000):
                if not self.is_speaking:
                    print("å¼€å§‹è¯´è¯")
                    self.is_speaking = True
                self.recording.append(data)
            else:
                if self.is_speaking:
                    print("åœæ­¢è¯´è¯")
                    self.is_speaking = False
                    self.chat(b''.join(self.recording))
                    self.recording = []
                    time.sleep(1)  # ç­‰å¾…1ç§’ä»¥é¿å…è¿ç»­è§¦å‘

    def chat(self, audio_data):
        self.save_recording_as_wav(audio_data)
        audio_file = sr.AudioFile('user_input.wav')
        with audio_file as source:
            audio = self.recognizer.record(source)
        # ä½¿ç”¨è¯­éŸ³è¯†åˆ«å™¨å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æœ¬
        try:
            text = self.recognizer.recognize_google(audio)
            print(f"è¯†åˆ«åˆ°çš„æ–‡æœ¬ï¼š{text}")
            if "å…³æœº" in text:
                print("æ­£åœ¨å…³é—­...")
                self.stream.stop_stream()
                self.stream.close()
                self.p.terminate()
                exit()
        except sr.UnknownValueError:
            print("æ— æ³•è¯†åˆ«éŸ³é¢‘")
        except sr.RequestError as e:
            print(f"è¯·æ±‚é”™è¯¯ï¼š{e}")
        # åŒæ—¶æ‹æ‘„å›¾ç‰‡å¹¶ä¸Šä¼ 
        chat_img_url = self.get_and_upload_img()
        prompt = self.create_prompt(text, "AGV_arm")
        # è°ƒç”¨Geminiå¤„ç†å¯¹è¯
        # ä½¿ç”¨mem0å¤„ç†è®°å¿†å±‚
        completion = self.openrouter_client.chat.completions.create(
        model="google/gemini-2.0-flash-thinking-exp:free",
        messages=[
            {
            "role": "user",
            "content": [
                {
                "type": "text",
                "text": prompt,
                },
                {
                "type": "image_url",
                "image_url": {
                    "url": chat_img_url,
                }
                }
            ]
            }
        ]
        )
        print(completion.choices[0].message.content)
        return completion.choices[0].message.content
    def save_recording_as_wav(self, audio_data):
        # åˆ›å»º WAV æ–‡ä»¶
        wf = wave.open("user_input.wav", 'wb')
        # è®¾ç½® WAV æ–‡ä»¶çš„å‚æ•°
        wf.setnchannels(1)  # å•å£°é“
        wf.setsampwidth(self.p.get_sample_size(pyaudio.paInt16))  # é‡‡æ ·å®½åº¦
        wf.setframerate(16000)  # é‡‡æ ·ç‡
        # å†™å…¥éŸ³é¢‘æ•°æ®
        wf.writeframes(audio_data)
        # å…³é—­ WAV æ–‡ä»¶
        wf.close()
        print(f"Audio saved to user_input.wav")
    
    # è°ƒç”¨client_mem0ç”Ÿæˆæç¤ºè¯
    def create_prompt(self, user_input, user_id):
        # search for memories related to the user's input
        memories = self.mem0_client.search(user_input, user_id="AGV_arm")
        context = "\n".join([m["memory"] for m in memories])
        
        # make user's input part of the memory
        self.mem0_client.add(user_input, user_id="AGV_arm")
        prompt = f"""You are EVA. The user is your sincere friend, and you are a helpful assistant. You have an arm and can help the user with tasks. You can add some icon to more precisely express your emotion. Such as ğŸ˜Š, ğŸ˜¢, ğŸ˜¡, ğŸ˜±, ğŸ˜, etc.
        Memories:
        {context}
        User's name is {user_id}
        User's input: {user_input}
        """
        
    # æ¯æ¬¡è°ƒç”¨chatï¼Œéƒ½ä¼šåŒæ—¶è¿›è¡Œå›¾ç‰‡æ‹æ‘„å’Œä¸Šä¼ 
    def get_and_upload_img(self):
        img_path = "image.png"
        print("Capturing image")
        self.camera.CaptureImg(img_path)
        print("Image captured")
        from data_mem import supabase_processor
        img_url = supabase_processor().upload_img()
        print("Image uploaded")
        return img_url
        

# è·å–USBå£°å¡çš„è®¾å¤‡ç´¢å¼•
def get_usb_device_index():
    p = pyaudio.PyAudio()
    info = p.get_host_api_info_by_index(0)
    numdevices = info.get('deviceCount')
    for i in range(0, numdevices):
        if (p.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:
            print("Input Device id ", i, " - ", p.get_device_info_by_host_api_device_index(0, i).get('name'))
            if "USB" in p.get_device_info_by_host_api_device_index(0, i).get('name'):
                return i
    print("æ²¡æœ‰æ‰¾åˆ°USBå£°å¡è®¾å¤‡")
    return None

if __name__ == "__main__":
    device_index = get_usb_device_index()
    if device_index is None:
        exit()
    conv = Conversation(device_index)
    conv.listen()





